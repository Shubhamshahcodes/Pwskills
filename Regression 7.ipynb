{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d2dd6-9242-4c8c-b586-f42e7b25bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. \n",
    "R-squared (R^2) is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model. It is calculated as the ratio of the explained variance to the total variance in the data. R-squared values range from 0 to 1, where 0 indicates that the model does not explain any variance in the dependent variable, and 1 indicates that the model explains all the variance.\n",
    "\n",
    "# Q2. \n",
    "Adjusted R-squared is a modified version of the regular R-squared that adjusts for the number of predictors in the model. Unlike R-squared, which can artificially increase with the addition of more predictors, adjusted R-squared penalizes the addition of unnecessary predictors by taking into account the degrees of freedom. Adjusted R-squared can be lower than R-squared when additional predictors do not improve the model significantly.\n",
    "\n",
    "# Q3. \n",
    "Adjusted R-squared is more appropriate to use when comparing models with different numbers of predictors. It provides a more accurate measure of the model's goodness of fit by penalizing overfitting due to the inclusion of unnecessary predictors.\n",
    "\n",
    "# Q4. \n",
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used evaluation metrics in regression analysis:\n",
    "- RMSE is the square root of the average of squared differences between the predicted and actual values.\n",
    "- MSE is the average of squared differences between the predicted and actual values.\n",
    "- MAE is the average of absolute differences between the predicted and actual values.\n",
    "\n",
    "# Q5. Advantages of RMSE, MSE, and MAE:\n",
    "- They provide a quantitative measure of the model's prediction accuracy.\n",
    "- They penalize large errors more than small errors, making them sensitive to outliers.\n",
    "\n",
    "Disadvantages:\n",
    "- They do not provide direct information about the model's goodness of fit.\n",
    "- RMSE and MSE are influenced by the scale of the dependent variable, making them less interpretable.\n",
    "\n",
    "# Q6. \n",
    "Lasso regularization, or L1 regularization, adds a penalty term to the regression objective function that is proportional to the absolute value of the coefficients. It encourages sparsity in the coefficient values, effectively performing feature selection by shrinking some coefficients to zero. Lasso regularization differs from Ridge regularization (L2 regularization), which penalizes the sum of squared coefficients.\n",
    "\n",
    "# Q7. \n",
    "Regularized linear models help prevent overfitting by penalizing large coefficient values, thereby reducing the model's complexity. For example, in Lasso regularization, some coefficients are shrunk to zero, effectively eliminating the corresponding features from the model. This prevents the model from learning noise or irrelevant patterns in the data, leading to improved generalization performance on unseen data.\n",
    "\n",
    "# Q8. Limitations of regularized linear models:\n",
    "- They require careful selection of the regularization parameter, which can be challenging and may require tuning through cross-validation.\n",
    "- They may not perform well when there is multicollinearity among the predictors, as regularization may arbitrarily shrink coefficients.\n",
    "- They may not capture complex non-linear relationships between the predictors and the target variable.\n",
    "\n",
    "# Q9. In this scenario, Model B with an MAE of 8 would be considered the better performer. MAE is less sensitive to outliers compared to RMSE, making it a more robust metric. However, the choice of metric depends on the specific characteristics of the data and the problem domain. Limitations include the fact that MAE treats all errors equally and does not penalize larger errors more than smaller ones.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
