{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd4f4d5-2b4d-4768-a3ed-49b851525e64",
   "metadata": {},
   "source": [
    "# Q1: \n",
    "\n",
    "The Filter method in feature selection involves evaluating the relevance of features independently of the machine learning model. It works by applying statistical techniques or metrics to each feature individually and ranking them based on their scores. Common techniques used in the Filter method include correlation analysis, chi-square test, mutual information, and variance thresholding. Features are then selected or eliminated based on their scores, without considering their interactions with other features or their impact on the final model.\n",
    "\n",
    "# Q2: \n",
    "\n",
    "The Wrapper method differs from the Filter method in that it evaluates the performance of different subsets of features by training and testing the machine learning model iteratively. It works by creating subsets of features, training the model on each subset, and evaluating its performance using cross-validation or a validation set. The Wrapper method considers the interaction between features and the model's performance, which allows it to select the best subset of features for the specific model being used.\n",
    "\n",
    "# Q3: Some common techniques used in Embedded feature selection methods include:\n",
    "\n",
    "Lasso (L1 regularization): Penalizes the absolute size of the coefficients during model training, effectively shrinking some coefficients to zero and selecting the most important features.\n",
    "\n",
    "Ridge (L2 regularization): Penalizes the squared size of the coefficients during model training, which can also lead to feature selection by shrinking less important coefficients.\n",
    "\n",
    "Decision tree-based methods: Decision trees inherently perform feature selection by splitting nodes based on the most informative features, and ensemble methods like Random Forest and Gradient Boosting Machines can further refine feature importance estimates.\n",
    "\n",
    "# Q4: Some drawbacks of using the Filter method for feature selection include:\n",
    "\n",
    "Lack of consideration for feature interactions: The Filter method evaluates features independently and may not capture complex interactions between features that are important for the model.\n",
    "\n",
    "Limited to predefined metrics: The Filter method relies on predefined statistical metrics or scores, which may not always capture the full complexity of the data or the model.\n",
    "\n",
    "Insensitivity to model performance: The Filter method does not directly consider the impact of feature selection on the model's performance, which may lead to suboptimal feature subsets.\n",
    "\n",
    "# Q5: The Filter method is preferred over the Wrapper method for feature selection in situations where:\n",
    "\n",
    "There is a large number of features relative to the size of the dataset.\n",
    "\n",
    "Computational resources are limited, as the Filter method is computationally less expensive than the Wrapper method.\n",
    "\n",
    "Feature interactions are not expected to play a significant role in the model's performance, and independent feature evaluation suffices.\n",
    "\n",
    "# Q6: To choose the most pertinent attributes for the predictive model of customer churn using the Filter Method in the telecom company project, I would follow these steps:\n",
    "\n",
    "Compute relevant statistical metrics for each feature, such as correlation coefficients or mutual information scores, with the target variable (customer churn).\n",
    "\n",
    "Rank the features based on their scores and select the top-ranked features as potential attributes for the model.\n",
    "\n",
    "Perform further analysis, such as visualization or domain knowledge exploration, to validate the selected features and ensure their relevance to predicting customer churn.\n",
    "\n",
    "Iterate on the feature selection process if necessary, considering the trade-offs between model performance and feature complexity.\n",
    "\n",
    "# Q7: To select the most relevant features for predicting the outcome of a soccer match using the Embedded method, I would follow these steps:\n",
    "\n",
    "Train a machine learning model on the entire dataset, including all features related to player statistics and team rankings.\n",
    "\n",
    "Use techniques such as Lasso regularization or decision tree-based methods to estimate feature importance or coefficients.\n",
    "\n",
    "Analyze the resulting feature importance scores to identify the most relevant features for predicting the outcome of soccer matches.\n",
    "\n",
    "Select the top-ranked features based on their importance scores and use them to train the final predictive model.\n",
    "\n",
    "# Q8: To select the best set of features for predicting the price of a house using the Wrapper method, I would follow these steps:\n",
    "\n",
    "Define a set of candidate features based on domain knowledge and data availability, including features such as size, location, and age of the house.\n",
    "\n",
    "Generate different subsets of features using techniques such as forward selection, backward elimination, or recursive feature elimination.\n",
    "\n",
    "Train and evaluate a machine learning model, such as a regression model, on each subset of features using cross-validation or a validation set.\n",
    "\n",
    "Select the subset of features that maximizes the model's performance metric, such as R-squared or mean squared error, on the validation data.\n",
    "\n",
    "Use the selected features to train the final predictive model for predicting the price of houses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
