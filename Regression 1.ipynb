{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9ffef0-0b90-448e-90d3-e36194912b32",
   "metadata": {},
   "source": [
    "# Q1. Simple Linear Regression vs. Multiple Linear Regression:\n",
    "\n",
    "Simple linear regression involves predicting a dependent variable using only one independent variable. For example, predicting house prices based on the area of the house.\n",
    "Multiple linear regression extends simple linear regression to include multiple independent variables in predicting the dependent variable. For example, predicting house prices based on area, number of bedrooms, and location.\n",
    "\n",
    "\n",
    "# Q2. Assumptions of Linear Regression:\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables should be linear.\n",
    "Independence: The residuals (errors) should be independent of each other.\n",
    "Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variable(s).\n",
    "Normality: The residuals should be normally distributed.\n",
    "To check these assumptions, diagnostic plots such as residual plots, QQ plots, and scatter plots can be used.\n",
    "\n",
    "# Q3. Interpretation of Slope and Intercept:\n",
    "\n",
    "Slope (Coefficient): It represents the change in the dependent variable for a one-unit change in the independent variable, holding other variables constant.\n",
    "Intercept: It represents the value of the dependent variable when all independent variables are zero. In practical terms, it might not always have a meaningful interpretation.\n",
    "Example: In a linear regression model predicting exam scores based on study hours, the slope indicates how much the exam score is expected to increase (or decrease) for each additional hour of study, while the intercept represents the expected exam score when the study hours are zero.\n",
    "\n",
    "# Q4. Gradient Descent:\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function (error) in machine learning models.\n",
    "It works by iteratively adjusting model parameters (coefficients) in the direction of steepest descent of the cost function.\n",
    "This process continues until the algorithm converges to the minimum point of the cost function, thereby optimizing the model parameters.\n",
    "\n",
    "# Q5. Multiple Linear Regression Model:\n",
    "\n",
    "Multiple linear regression involves predicting a dependent variable using multiple independent variables.\n",
    "Mathematically, it can be represented as: Y = β0 + β1X1 + β2X2 + ... + βnXn + ε, where Y is the dependent variable, X1, X2, ..., Xn are the independent variables, β0, β1, ..., βn are the coefficients, and ε is the error term.\n",
    "It differs from simple linear regression by allowing for multiple predictors to be included in the model.\n",
    "\n",
    "# Q6. Multicollinearity in Multiple Linear Regression:\n",
    "\n",
    "Multicollinearity occurs when independent variables in a regression model are highly correlated with each other.\n",
    "This can lead to unstable parameter estimates and difficulty in interpreting the coefficients.\n",
    "To detect multicollinearity, one can calculate correlation coefficients between independent variables or use variance inflation factor (VIF).\n",
    "Addressing multicollinearity can involve removing one of the correlated variables, transforming variables, or using regularization techniques like ridge regression.\n",
    "\n",
    "# Q7. Polynomial Regression Model:\n",
    "\n",
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial.\n",
    "Unlike linear regression, which assumes a linear relationship between variables, polynomial regression can capture non-linear relationships.\n",
    "It can be represented as: Y = β0 + β1X + β2X^2 + ... + βnX^n + ε, where X^2, X^3, ..., X^n represent higher-order terms of the independent variable.\n",
    "\n",
    "# Q8. Advantages and Disadvantages of Polynomial Regression:\n",
    "\n",
    "Advantages:\n",
    "Can model complex relationships that linear regression cannot capture.\n",
    "Provides better fit to non-linear data.\n",
    "Disadvantages:\n",
    "More prone to overfitting, especially with higher degree polynomials.\n",
    "Interpretability can be challenging with higher degree polynomials.\n",
    "Polynomial regression is preferred when the relationship between variables is non-linear and linear regression does not provide an adequate fit. However, caution must be exercised to prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
