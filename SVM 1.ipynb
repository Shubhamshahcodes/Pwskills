{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe256d79-c9fb-40e7-877a-fda88eb4bf58",
   "metadata": {},
   "source": [
    "# Q1. The mathematical formula for a linear Support Vector Machine (SVM) is:\n",
    "\n",
    "f(x) = sign(w ‚ãÖ x + b)\n",
    "\n",
    "where:\n",
    "\n",
    "- f(x) is the decision function that predicts the class of a new input x,\n",
    "- w is the weight vector,\n",
    "- b is the bias term,\n",
    "- x is the input features,\n",
    "- ‚ãÖ denotes the dot product,\n",
    "- sign is the sign function that returns -1 for negative values and +1 for positive values.\n",
    "\n",
    "# Q2. The objective function of a linear SVM is to maximize the margin between the decision boundary (hyperplane) and the closest data points (support vectors) while minimizing the classification error. Mathematically, it can be expressed as:\n",
    "\n",
    "minimize w, b 1/2 ||w||^2\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "yi(w ‚ãÖ xi + b) ‚â• 1\n",
    "\n",
    "where xi are the training samples, yi are their corresponding class labels (+1 or -1), w is the weight vector, and b is the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ba343-c231-4808-9b48-18fc5b5d1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. \n",
    "The kernel trick in SVM allows for nonlinear decision boundaries by implicitly mapping the input features into a higher-dimensional space where the data may be separable by a hyperplane. Instead of computing the dot product in the original feature space, the kernel function computes the dot product in the higher-dimensional space efficiently. This enables SVM to learn complex decision boundaries without explicitly transforming the input features.\n",
    "\n",
    "# Q4. Support vectors are the data points that lie closest to the decision boundary (hyperplane) and have a non-zero value for the Lagrange multiplier (alpha) in the optimization problem. These points determine the position and orientation of the hyperplane and are crucial for defining the margin. They are called \"support\" vectors because they support the hyperplane. Support vectors play a significant role in the generalization ability of the SVM model as they capture the most informative data points.\n",
    "\n",
    "For example, consider a binary classification problem with two classes, represented by red and blue points. In the figure below, the support vectors are the data points located on or near the margin boundary (dashed lines). These support vectors determine the position and orientation of the decision boundary (solid line).\n",
    "\n",
    "\n",
    "Q5. Illustration with examples and graphs:\n",
    "\n",
    "Hyperplane: In SVM, the hyperplane is the decision boundary that separates the classes. For a two-dimensional feature space, the hyperplane is a line. In three dimensions, it becomes a plane, and in higher dimensions, it's a hyperplane. The goal of SVM is to find the hyperplane that maximizes the margin between the classes. Below is an example of a hyperplane separating two classes (red and blue points) in a two-dimensional feature space.\n",
    "Marginal Plane: The marginal planes in SVM are parallel hyperplanes that run parallel to the decision boundary (hyperplane) and define the margin. They are equidistant from the hyperplane and serve as the boundaries of the margin. The margin is the region between the marginal planes where no data points lie. The support vectors lie on the marginal planes or within the margin. Below is an illustration of the marginal planes (dashed lines) and the margin (shaded region) for a two-class problem.\n",
    "Soft Margin: In SVM, the soft margin allows for some misclassification errors by allowing data points to fall within the margin or on the wrong side of the margin boundary. It introduces a penalty for misclassification, controlled by the regularization parameter \n",
    "ùê∂\n",
    "C. Soft margin SVM is suitable for data that is not perfectly separable, as it trades off margin size with misclassification error. Below is an example of a soft margin SVM with some misclassified points (red circles) within the margin.\n",
    "Hard Margin: In contrast, the hard margin SVM requires all data points to be correctly classified and lie outside the margin boundary. It only works when the data is linearly separable, and there are no misclassification errors allowed. Hard margin SVM can be sensitive to outliers and noise in the data. Below is an example of a hard margin SVM where all data points are correctly classified, and there are no misclassified points within the margin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
