{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6ab29d-c1bb-45dd-a51c-2f2a436d3124",
   "metadata": {},
   "source": [
    "# Q1. \n",
    "Linear regression is used for predicting continuous outcomes based on one or more predictor variables. It models the relationship between the independent variables and the dependent variable using a linear equation. In contrast, logistic regression is used for binary classification problems where the outcome variable is categorical and has only two possible outcomes. Logistic regression models the probability of the binary outcome as a function of the independent variables using the logistic function (sigmoid function). \n",
    "\n",
    "Example scenario: Predicting whether an email is spam or not spam based on features like sender, subject line, and content. Here, logistic regression would be more appropriate as the outcome variable (spam or not spam) is binary.\n",
    "\n",
    "# Q2. \n",
    "In logistic regression, the cost function used is the binary cross-entropy loss (log loss) function. It quantifies the difference between the predicted probabilities and the actual binary outcomes. The optimization process aims to minimize this cost function using techniques like gradient descent.\n",
    "\n",
    "# Q3. \n",
    "Regularization in logistic regression involves adding penalty terms to the cost function to discourage large coefficients, thus preventing overfitting. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge).\n",
    "\n",
    "#Q4. \n",
    "The ROC (Receiver Operating Characteristic) curve is a graphical plot that illustrates the performance of a binary classification model across different thresholds. It plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity). The area under the ROC curve (AUC-ROC) is a commonly used metric to evaluate the overall performance of the logistic regression model.\n",
    "\n",
    "# Q5. \n",
    "Common techniques for feature selection in logistic regression include:\n",
    "   - Univariate feature selection: Selecting features based on statistical tests like chi-square test, ANOVA, or correlation coefficients.\n",
    "   - Recursive feature elimination: Iteratively removing the least important features based on model performance.\n",
    "   - Regularization: Penalizing large coefficients using L1 (Lasso) or L2 (Ridge) regularization.\n",
    "\n",
    "These techniques help improve the model's performance by reducing overfitting, improving interpretability, and enhancing computational efficiency.\n",
    "\n",
    "# Q6. \n",
    "Imbalanced datasets in logistic regression occur when one class dominates the other in terms of the number of samples. Strategies for handling class imbalance include:\n",
    "   - Resampling techniques: Oversampling the minority class (e.g., SMOTE) or undersampling the majority class to balance the dataset.\n",
    "   - Algorithmic techniques: Using algorithms that handle class imbalance better, such as ensemble methods like Random Forest or boosting algorithms like XGBoost.\n",
    "   - Cost-sensitive learning: Assigning higher misclassification costs to the minority class to focus the model on learning it better.\n",
    "\n",
    "# Q7. \n",
    "Common issues in logistic regression include multicollinearity among independent variables, which can inflate standard errors and lead to unstable coefficient estimates. This can be addressed by removing highly correlated predictors, using dimensionality reduction techniques like PCA, or employing regularization techniques like Ridge regression that mitigate multicollinearity. Additionally, logistic regression assumes linearity between the independent variables and the log odds of the outcome, so nonlinear relationships may require transformations or alternative modeling approaches. Finally, logistic regression can be sensitive to outliers, which may need to be addressed through data preprocessing or robust regression methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
