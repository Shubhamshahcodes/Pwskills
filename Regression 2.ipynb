{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c38022-323c-4b30-9c0e-3fd1db5b253b",
   "metadata": {},
   "source": [
    "# Q1. R-squared (Coefficient of Determination) in Linear Regression:\n",
    "\n",
    "R-squared is a statistical measure that represents the proportion of variance in the dependent variable that is explained by the independent variables in a regression model.\n",
    "It is calculated as the ratio of the sum of squares of the fitted values to the total sum of squares of the dependent variable.\n",
    "R-squared values range from 0 to 1, where 0 indicates that the model does not explain any variance in the dependent variable, and 1 indicates a perfect fit where all variance is explained.\n",
    "\n",
    "# Q2. Adjusted R-squared:\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that penalizes the inclusion of additional independent variables in the model.\n",
    "It adjusts for the number of predictors in the model and provides a more accurate measure of model fit, especially in the presence of multiple independent variables.\n",
    "Unlike R-squared, adjusted R-squared can decrease when adding irrelevant variables to the model.\n",
    "\n",
    "# Q3. Appropriateness of Adjusted R-squared:\n",
    "\n",
    "Adjusted R-squared is more appropriate when comparing the fit of models with different numbers of predictors.\n",
    "It helps prevent overfitting by penalizing the inclusion of unnecessary variables in the model.\n",
    "\n",
    "# Q4. RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error):\n",
    "\n",
    "RMSE is the square root of the average of the squared differences between predicted and actual values.\n",
    "MSE is the average of the squared differences between predicted and actual values.\n",
    "MAE is the average of the absolute differences between predicted and actual values.\n",
    "These metrics quantify the accuracy of a regression model by measuring the differences between predicted and actual values.\n",
    "\n",
    "# Q5. Advantages and Disadvantages of RMSE, MSE, and MAE:\n",
    "\n",
    "Advantages:\n",
    "Provide a quantitative measure of prediction error.\n",
    "Easy to interpret and calculate.\n",
    "Disadvantages:\n",
    "Sensitive to outliers, especially RMSE and MSE.\n",
    "Do not provide information about the direction of errors.\n",
    "May not directly reflect model interpretability or utility.\n",
    "\n",
    "# Q6. Lasso Regularization:\n",
    "\n",
    "Lasso regularization (L1 regularization) adds a penalty term to the linear regression cost function equal to the absolute value of the coefficients multiplied by a regularization parameter (λ).\n",
    "It encourages sparsity by shrinking less important coefficients to zero, effectively performing feature selection.\n",
    "Lasso differs from Ridge regularization in that it can lead to feature selection by setting some coefficients exactly to zero.\n",
    "\n",
    "# Q7. Prevention of Overfitting with Regularized Linear Models:\n",
    "\n",
    "Regularized linear models like Lasso and Ridge regression help prevent overfitting by penalizing large coefficients and reducing model complexity.\n",
    "By shrinking coefficients towards zero, these regularization techniques limit the model's flexibility and make it less sensitive to noise in the training data.\n",
    "For example, in Ridge regression, the penalty term in the cost function helps to smooth out the model and prevent it from fitting the noise in the data too closely.\n",
    "\n",
    "# Q8. Limitations of Regularized Linear Models:\n",
    "\n",
    "Regularized linear models assume a linear relationship between the independent and dependent variables, which may not always hold true in real-world data.\n",
    "The choice of the regularization parameter (λ) is critical and often requires tuning through cross-validation.\n",
    "These models may not perform well if the assumptions of linear regression are violated or if there is multicollinearity among the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d724f-129a-4886-9ca3-d0dd0475b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. For regression models, RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) are both measures of prediction error. RMSE penalizes large errors more heavily, while MAE treats all errors equally. The choice between models depends on the context and the importance of different error sizes.\n",
    "\n",
    "Q10. When comparing regularized linear models, Ridge regularization is preferable when dealing with multicollinearity, as it shrinks coefficients without setting them to zero. Lasso regularization, on the other hand, is useful for feature selection by setting some coefficients to zero, making it suitable for datasets with many irrelevant predictors. The choice between models depends on the dataset characteristics and the trade-offs between model complexity and interpretability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
