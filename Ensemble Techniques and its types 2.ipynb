{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c783fe59-acca-4e62-8db5-fa53a6551b48",
   "metadata": {},
   "source": [
    "# Q1. \n",
    "Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by training multiple trees on different subsets of the training data, obtained through bootstrap sampling. By averaging the predictions of multiple trees, bagging reduces the variance of the model, making it less prone to overfitting.\n",
    "\n",
    "# Q2. \n",
    "Advantages of using different types of base learners in bagging include increased diversity in the ensemble, which can lead to improved generalization performance. However, the disadvantage is that heterogeneous base learners may introduce more computational complexity and may require additional hyperparameter tuning.\n",
    "\n",
    "# Q3. \n",
    "The choice of base learner can affect the bias-variance tradeoff in bagging. Using a base learner with high bias (e.g., a shallow decision tree) may result in an ensemble with high bias but low variance, while using a base learner with low bias (e.g., a deep decision tree) may result in an ensemble with low bias but high variance.\n",
    "\n",
    "# Q4. \n",
    "Yes, bagging can be used for both classification and regression tasks. In classification tasks, bagging typically involves training an ensemble of classifiers (e.g., decision trees) and using voting or averaging to make predictions. In regression tasks, bagging involves training an ensemble of regression models and averaging their predictions.\n",
    "\n",
    "# Q5. \n",
    "The ensemble size in bagging refers to the number of base learners included in the ensemble. Generally, increasing the ensemble size can improve the performance of the bagging algorithm up to a certain point, after which the marginal benefit may decrease. The optimal ensemble size depends on factors such as the complexity of the problem and the computational resources available.\n",
    "\n",
    "# Q6. \n",
    "A real-world application of bagging in machine learning is in the field of finance for predicting stock prices. Multiple regression models or decision trees can be trained on historical stock data, and bagging can be used to combine their predictions to create a more robust model for predicting future stock prices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
