{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd1b108-0e22-4921-ac2f-00db5c6237a5",
   "metadata": {},
   "source": [
    "# Q1. \n",
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It is an ensemble method that builds a predictive model in the form of an ensemble of weak prediction models, typically decision trees. Gradient Boosting Regression sequentially trains multiple weak learners, each one focusing on the residuals (the difference between the actual and predicted values) of the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5aac7-4249-4a1b-a4c7-ffd81d97d7f4",
   "metadata": {},
   "source": [
    "# Q2. Here's a simple implementation of gradient boosting regression from scratch using Python and NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61f10ef-c03a-4ab0-9bb5-b6333a72f081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.338029663217839\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the residuals with the target values\n",
    "        residuals = np.copy(y)\n",
    "        \n",
    "        # Iterate over the number of estimators\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Predict the residuals for the current tree\n",
    "            residuals_pred = tree.predict(X)\n",
    "            \n",
    "            # Update the residuals with the negative gradient\n",
    "            residuals -= self.learning_rate * residuals_pred\n",
    "            \n",
    "            # Add the current tree to the ensemble\n",
    "            self.models.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Initialize predictions with zeros\n",
    "        predictions = np.zeros(len(X))\n",
    "        \n",
    "        # Iterate over the models and make predictions\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3bd6cd-b67f-41d2-99e4-8d2456fe012d",
   "metadata": {},
   "source": [
    "Q3. To experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the model, you can use grid search or random search techniques. Here's an example using scikit-learn's GridSearchCV for hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5186a4c-2d21-44eb-a327-61d95c261e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class GradientBoostingRegressor(BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the residuals with the target values\n",
    "        residuals = np.copy(y)\n",
    "        \n",
    "        # Iterate over the number of estimators\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Predict the residuals for the current tree\n",
    "            residuals_pred = tree.predict(X)\n",
    "            \n",
    "            # Update the residuals with the negative gradient\n",
    "            residuals -= self.learning_rate * residuals_pred\n",
    "            \n",
    "            # Add the current tree to the ensemble\n",
    "            self.models.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Initialize predictions with zeros\n",
    "        predictions = np.zeros(len(X))\n",
    "        \n",
    "        # Iterate over the models and make predictions\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        if 'n_estimators' in params:\n",
    "            self.n_estimators = params['n_estimators']\n",
    "        if 'learning_rate' in params:\n",
    "            self.learning_rate = params['learning_rate']\n",
    "        if 'max_depth' in params:\n",
    "            self.max_depth = params['max_depth']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5203f8-65d0-4968-9453-bd3a115cc64f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
